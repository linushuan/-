#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
STL ç©ºæ°£å“è³ªæ•¸æ“šå¾Œè™•ç†å·¥å…· (Step 2)
åŠŸèƒ½ï¼š
1. è‡ªå‹•è®€å– output_results_v3/anomaly_csvs ä¸­çš„è·å¹³æª”æ¡ˆã€‚
2. åŸ·è¡Œ STL åˆ†è§£ç§»é™¤æ—¥å¤œé€±æœŸ (Period=24)ã€‚
3. åš´æ ¼çš„æ–·é»è™•ç†ï¼š
   - ç¼ºæ¸¬ <= 2å°æ™‚ï¼šè‡ªå‹•è£œå€¼ (ç·šæ€§æ’å€¼)ã€‚
   - ç¼ºæ¸¬ > 2å°æ™‚ï¼šè¦–ç‚ºæ–·é» (ä¿ç•™ NaN)ï¼Œä¸¦ç”Ÿæˆç¨ç«‹å ±å‘Šã€‚
4. è¼¸å‡ºçµæœè‡³ output_results_v3/stl_processed_data èˆ‡ reportsã€‚
"""

import os
import glob
import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import STL
from tqdm import tqdm
import warnings

# å¿½ç•¥ Statsmodels åœ¨æŸäº›æ¥µç«¯æ•¸æ“šä¸‹çš„è­¦å‘Š
warnings.filterwarnings("ignore")

# ===============================================================
#   è·¯å¾‘è¨­å®š (æ ¹æ“šæ‚¨æä¾›çš„ç¨‹å¼ç¢¼é‚è¼¯)
# ===============================================================

current_dir = os.path.dirname(os.path.abspath(__file__))
base_output_root = os.path.join(current_dir, "output_results_v3_nodb/")

# è¼¸å…¥ï¼šä¸Šä¸€éšæ®µç”¢å‡ºçš„è·å¹³ CSV
INPUT_DIR = os.path.join(base_output_root, "anomaly_csvs")

# è¼¸å‡ºï¼šæ–°çš„è³‡æ–™å¤¾
OUTPUT_DATA_DIR = os.path.join(base_output_root, "stl_processed_data")
OUTPUT_REPORT_DIR = os.path.join(base_output_root, "stl_reports")

# å»ºç«‹è¼¸å‡ºç›®éŒ„
for d in [OUTPUT_DATA_DIR, OUTPUT_REPORT_DIR]:
    os.makedirs(d, exist_ok=True)

# ===============================================================
#   åƒæ•¸è¨­å®š
# ===============================================================

TOLERANCE_HOURS = 2       # å®¹å¿ç¼ºæ¸¬æ™‚æ•¸ (è¶…éæ­¤æ•¸å€¼å‰‡è¦–ç‚ºæ–·é»)
STL_PERIOD = 24           # é€±æœŸ (å°æ™‚è³‡æ–™ç‚º 24)
STL_SEASONAL = 13         # å­£ç¯€å¹³æ»‘åƒæ•¸ (é€šå¸¸ç‚ºå¥‡æ•¸ï¼Œ13 æ˜¯å¸¸ç”¨é è¨­å€¼)

# ===============================================================
#   æ ¸å¿ƒè™•ç†å‡½å¼
# ===============================================================

def process_series_stl(series, site, item, filename):
    """
    å°å–®ä¸€åºåˆ—é€²è¡Œï¼šç¼ºæ¸¬æª¢æŸ¥ -> æ’å€¼ -> STLåˆ†è§£ -> æ–·é»é‚„åŸ
    """
    gap_reports = []

    # 1. åµæ¸¬ NaN åˆ†ä½ˆ
    is_nan = series.isna()
    if is_nan.all():
        return series, gap_reports

    # æ‰¾å‡ºé€£çºŒ NaN çš„å€å¡Š
    # åˆ©ç”¨ ne() èˆ‡ cumsum() å¿«é€Ÿåˆ†çµ„
    nan_groups = is_nan.ne(is_nan.shift()).cumsum()
    nan_blocks = series[is_nan].groupby(nan_groups)

    for _, block in nan_blocks:
        gap_len = len(block)
        if gap_len > TOLERANCE_HOURS:
            # è¨˜éŒ„è¶…éå®¹å¿å€¼çš„ç¼ºæ¸¬
            start_t = block.index[0]
            end_t = block.index[-1]
            gap_reports.append({
                'file': filename,
                'site': site,
                'item': item,
                'type': 'é•·æ™‚é–“ç¼ºæ¸¬ (STLä¸­æ–·)',
                'duration_hours': gap_len,
                'start_time': start_t.strftime('%Y-%m-%d %H:%M'),
                'end_time': end_t.strftime('%Y-%m-%d %H:%M')
            })

    # 2. æ’å€¼è£œå€¼
    # limit=TOLERANCE_HOURS: åªè£œå°æ´ï¼Œå¤§æ´ç•™è‘— NaN
    series_interp = series.interpolate(method='linear', limit=TOLERANCE_HOURS)

    # 3. STL æº–å‚™
    # STL ä¸æ¥å— NaNï¼Œå°æ–¼å‰©ä¸‹çš„å¤§æ´ (è¶…é2å°æ™‚çš„)ï¼Œæˆ‘å€‘æš«æ™‚å¡« 0 (å‡è¨­è·å¹³ç‚º0æ˜¯å¹³å‡æ…‹)
    # ä»¥ä¾¿ç®—å‡ºæ•´é«”çš„ Trendã€‚ç®—å®Œå¾Œå¿…é ˆæŠŠé€™äº›æ´æŒ–å›ä¾†ã€‚
    mask_large_gaps = series_interp.isna()
    series_for_stl = series_interp.fillna(0)

    # 4. åŸ·è¡Œ STL
    # è³‡æ–™é•·åº¦è‡³å°‘è¦æ˜¯é€±æœŸçš„å…©å€æ‰èƒ½ç®—
    if len(series_for_stl) > STL_PERIOD * 2:
        try:
            stl = STL(series_for_stl, period=STL_PERIOD, seasonal=STL_SEASONAL)
            res = stl.fit()

            # æ ¸å¿ƒå…¬å¼ï¼šç§»é™¤å­£ç¯€æ€§ = åŸå§‹å€¼ - å­£ç¯€æ€§æˆåˆ†
            # é€™æ¨£ä¿ç•™äº† Trend (è¶¨å‹¢) + Residual (çªç™¼ç•°å¸¸)
            deseasonalized = series_for_stl - res.seasonal

            # 5. é‚„åŸå¤§æ–·é» (é¿å…åœ–è¡¨èª¤å°)
            final_series = deseasonalized.mask(mask_large_gaps)
        except Exception:
            # å¦‚æœæ•¸å­¸é‹ç®—å¤±æ•— (æ¥µå°‘è¦‹ï¼Œå¦‚æ•¸æ“šè®Šç•°æ•¸ç‚º0)ï¼Œé€€å›ä½¿ç”¨æ’å€¼å¾Œçš„åŸæ•¸æ“š
            final_series = series_interp
    else:
        final_series = series_interp

    return final_series, gap_reports

def process_file(file_path):
    file_name = os.path.basename(file_path)

    try:
        # è®€å–ä¸Šä¸€éšæ®µçš„ CSV
        df = pd.read_csv(file_path)

        # æ¬„ä½æª¢æŸ¥
        if 'anomaly' not in df.columns and 'value' in df.columns:
            target_col = 'value'
        elif 'anomaly' in df.columns:
            target_col = 'anomaly'
        else:
            print(f"âš ï¸ è·³é {file_name}: æ‰¾ä¸åˆ° anomaly æˆ– value æ¬„ä½")
            return

        # --- é—œéµä¿®æ­£ï¼šæ”¯æ´æ··åˆæ ¼å¼æ™‚é–“ ---
        df['datetime'] = pd.to_datetime(df['datetime'], format='mixed', errors='coerce')

        # ç§»é™¤æ™‚é–“è§£æå¤±æ•—çš„è¡Œ (é¿å…å¾Œé¢å ±éŒ¯)
        df = df.dropna(subset=['datetime'])

    except Exception as e:
        print(f"âŒ è®€å–éŒ¯èª¤ {file_name}: {e}")
        return

    # å»ºç«‹è©²æª”æ¡ˆçš„å®Œæ•´æ™‚é–“è»¸ (ç”¨æ–¼å°é½Š)
    min_dt = df['datetime'].min()
    max_dt = df['datetime'].max()
    full_range = pd.date_range(start=min_dt, end=max_dt, freq='H')

    processed_rows = []
    all_reports = []

    # é‡å°æ¯å€‹ [æ¸¬ç«™, æ¸¬é …] åˆ†çµ„è™•ç†
    grouped = df.groupby(['site', 'item'])

    for (site, item), group in grouped:
        # é‡å»ºç´¢å¼•ä»¥ç¢ºä¿æ™‚é–“é€£çºŒ (ç”¢ç”Ÿå¿…è¦çš„ NaNs)
        group = group.set_index('datetime')
        # æ¶ˆé™¤é‡è¤‡ç´¢å¼• (é¿å… reindex å ±éŒ¯)
        group = group[~group.index.duplicated(keep='first')]

        series = group[target_col].reindex(full_range)

        # åŸ·è¡Œ STL èˆ‡æ–·é»åˆ†æ
        stl_series, reports = process_series_stl(series, site, item, file_name)

        all_reports.extend(reports)

        # æ•´ç†çµæœ
        df_res = pd.DataFrame({
            'datetime': stl_series.index,
            'site': site,
            'item': item,
            'anomaly_stl': stl_series.values
        })
        processed_rows.append(df_res)

    # è¼¸å‡ºçµæœ
    if processed_rows:
        final_df = pd.concat(processed_rows)
        final_df = final_df.dropna(subset=['anomaly_stl'])

        save_path = os.path.join(OUTPUT_DATA_DIR, f"stl_{file_name}")
        final_df.to_csv(save_path, index=False)

    # è¼¸å‡ºå ±å‘Š
    if all_reports:
        report_df = pd.DataFrame(all_reports)
        report_save_path = os.path.join(OUTPUT_REPORT_DIR, f"gap_report_{file_name}")
        report_df.to_csv(report_save_path, index=False, encoding='utf-8-sig')

# ===============================================================
#   ä¸»ç¨‹å¼åŸ·è¡Œå€
# ===============================================================

if __name__ == "__main__":
    # æª¢æŸ¥è¼¸å…¥ç›®éŒ„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥ç›®éŒ„: {INPUT_DIR}")
        print("è«‹ç¢ºèªæ‚¨æ˜¯å¦å·²ç¶“åŸ·è¡Œéç¬¬ä¸€éšæ®µçš„ç¨‹å¼ï¼Œä¸¦ç”¢å‡ºäº† anomaly_csvs è³‡æ–™å¤¾ã€‚")
        exit()

    files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.csv")))

    if not files:
        print(f"âš ï¸  åœ¨ {INPUT_DIR} ä¸­æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆã€‚")
        exit()

    print(f"ğŸš€ é–‹å§‹ STL å¾Œè™•ç† (å»é™¤æ—¥å¤œå·®ç•°)...")
    print(f"ğŸ“‚ è®€å–ä¾†æº: {INPUT_DIR}")
    print(f"ğŸ“‚ æ•¸æ“šè¼¸å‡º: {OUTPUT_DATA_DIR}")
    print(f"ğŸ“‚ å ±å‘Šè¼¸å‡º: {OUTPUT_REPORT_DIR}")
    print("-" * 50)

    for f in tqdm(files, unit="file"):
        process_file(f)

    print("\nâœ… STL è™•ç†å®Œæˆï¼")
    print(f"è«‹è‡³ {OUTPUT_REPORT_DIR} æŸ¥çœ‹æ–·é»å ±å‘Šã€‚")
