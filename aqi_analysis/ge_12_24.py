#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é«˜æ•ˆèƒ½ç’°å¢ƒæ•¸æ“šè·å¹³åˆ†æå·¥å…· (v3_NoDB - ç„¡è³‡æ–™åº«ç‰ˆ)
æ”¹é€²å…§å®¹ï¼š
 1. ç§»é™¤æ‰€æœ‰è³‡æ–™åº« (SQLite) å¯«å…¥æ“ä½œï¼Œç´”ç²¹é€²è¡Œæª”æ¡ˆèˆ‡åœ–è¡¨è¼¸å‡ºã€‚
 2. ä¿ç•™æ‰€æœ‰ v3 çš„ç¹ªåœ–èˆ‡å ±å‘Šå¢å¼·åŠŸèƒ½ã€‚
"""

import os
import glob
import time
import gc
import pandas as pd
import numpy as np
import matplotlib as mpl
mpl.use('Agg')
mpl.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.font_manager import fontManager, FontProperties
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# ===============================================================
#   CONFIG è¨­å®šå€
# ===============================================================

N_WORKERS = 6
# DB_PATH å·²ç§»é™¤

# è·¯å¾‘è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.join(current_dir, "data")
output_root = os.path.join(current_dir, "output_results_v3_nodb") # æ”¹åå€éš”
anomaly_dir = os.path.join(output_root, "anomaly_csvs")
img_dir = os.path.join(output_root, "anomaly_pictures")
report_dir = os.path.join(output_root, "reports")

for d in [base_dir, anomaly_dir, img_dir, report_dir]:
    os.makedirs(d, exist_ok=True)

# å€åŸŸå®šç¾©
areas = {
    "åŒ—": ["ä¸‰é‡", "ä¸­å£¢", "ä¸­å±±", "å†¬å±±", "å¤äº­", "åœŸåŸ", "åŸºéš†", "å£«æ—",
           "å¤§åœ’", "å®œè˜­", "å¹³é®", "æ–°åº—", "æ–°ç«¹", "æ–°èŠ", "æ¾å±±", "æ¿æ©‹",
           "æ—å£", "æ¡ƒåœ’", "æ°¸å’Œ", "æ±æ­¢", "æ·¡æ°´", "æ¹–å£", "ç«¹æ±", "èœå¯®",
           "è¬è¯", "è¬é‡Œ", "è§€éŸ³", "é™½æ˜", "é¾æ½­"],
    "ä¸­": ["ä¸‰ç¾©", "äºŒæ—", "å—æŠ•", "å¤§é‡Œ", "å½°åŒ–", "å¿ æ˜", "æ²™é¹¿",
           "ç·šè¥¿", "è‹—æ —", "è¥¿å±¯", "è±åŸ", "é ­ä»½"],
    "å—": ["ä»æ­¦", "å‰é‡‘", "å‰é®", "å–„åŒ–", "å˜‰ç¾©", "å¤§å¯®", "å®‰å—",
           "å°æ¸¯", "å±æ±", "å´™èƒŒ", "å·¦ç‡Ÿ", "å¾©èˆˆ", "æ†æ˜¥", "æ–—å…­",
           "æ–°æ¸¯", "æ–°ç‡Ÿ", "æœ´å­", "æ—åœ’", "æ¥ æ¢“", "æ©‹é ­", "æ½®å·",
           "ç¾æ¿ƒ", "è‡ºå—", "è‡ºè¥¿", "é³³å±±"],
    "æ±": ["è‡ºæ±", "èŠ±è“®"]
}

site_to_region = {}
for region, sites in areas.items():
    for site in sites:
        site_to_region[site] = region

# å®¹éŒ¯ç¯„åœè¨­å®š
items_info = {
    "AMB_TEMP": {"name": "ç’°å¢ƒæº«åº¦", "unit": "Â°C", "min": -15, "max": 55},
    "CO": {"name": "ä¸€æ°§åŒ–ç¢³", "unit": "ppm", "min": 0, "max": 60},
    "NO": {"name": "ä¸€æ°§åŒ–æ°®", "unit": "ppb", "min": 0, "max": 600},
    "NO2": {"name": "äºŒæ°§åŒ–æ°®", "unit": "ppb", "min": 0, "max": 600},
    "NOx": {"name": "æ°®æ°§åŒ–ç‰©", "unit": "ppb", "min": 0, "max": 1200},
    "O3": {"name": "è‡­æ°§", "unit": "ppb", "min": 0, "max": 600},
    "PM10": {"name": "PM10", "unit": "Î¼g/mÂ³", "min": 0, "max": 1200},
    "PM2.5": {"name": "PM2.5", "unit": "Î¼g/mÂ³", "min": 0, "max": 600},
    "RAINFALL": {"name": "é™é›¨é‡", "unit": "mm", "min": 0, "max": 3000},
    "RH": {"name": "ç›¸å°æ¿•åº¦", "unit": "%", "min": 0, "max": 100},
    "SO2": {"name": "äºŒæ°§åŒ–ç¡«", "unit": "ppb", "min": 0, "max": 300},
    "WD_HR": {"name": "å¹³å‡é¢¨å‘", "unit": "degrees", "min": 0, "max": 360},
    "WIND_DIREC": {"name": "é¢¨å‘", "unit": "degrees", "min": 0, "max": 360},
    "WIND_SPEED": {"name": "é¢¨é€Ÿ", "unit": "m/s", "min": 0, "max": 120},
    "WS_HR": {"name": "å¹³å‡é¢¨é€Ÿ", "unit": "m/s", "min": 0, "max": 120}
}

def set_chinese_font():
    p = "/usr/share/fonts/noto-cjk/NotoSansCJK-Regular.ttc"
    try:
        fontManager.addfont(p)
        prop = FontProperties(fname=p)
        if prop.get_name():
            mpl.rcParams['font.family'] = 'sans-serif'
            mpl.rcParams['font.sans-serif'] = [prop.get_name()]
    except Exception:
        pass
set_chinese_font()

# ===============================================================
#   STEP 1: é è™•ç†æ­·å²å¹³å‡å€¼ (ç„¡ DB å¯«å…¥)
# ===============================================================

global_avg_lookup = None

def load_and_transform_averages():
    print("ğŸ”„ è¼‰å…¥æ­·å²å¹³å‡å€¼ (è¨˜æ†¶é«”æ¨¡å¼)...")
    all_avg_list = []

    for item in items_info.keys():
        avg_file = os.path.join(base_dir, f"{item.lower()}_hourly_avg_fast.csv")
        if not os.path.exists(avg_file): continue
        try:
            df = pd.read_csv(avg_file)
            if 'æ¸¬ç«™' in df.columns: df = df.rename(columns={'æ¸¬ç«™': 'site'})
            if 'site' not in df.columns: continue

            df_melted = df.melt(id_vars=['site'], var_name='day_hour', value_name='avg_value')
            temp_split = df_melted['day_hour'].str.split('_', expand=True)

            df_melted['day_of_year'] = temp_split[0].astype('int16')
            df_melted['hour'] = temp_split[1].astype('int8')
            df_melted['item'] = item
            df_melted['avg_value'] = pd.to_numeric(df_melted['avg_value'], errors='coerce').astype('float32')

            df_melted = df_melted.dropna(subset=['avg_value'])
            all_avg_list.append(df_melted[['item', 'site', 'day_of_year', 'hour', 'avg_value']])

        except Exception as e:
            print(f"Error loading avg {item}: {e}")

    if not all_avg_list: return None

    print("âš¡ åˆä½µè¨˜æ†¶é«”ä¸­çš„æ­·å²ç´¢å¼•...")
    df_lookup = pd.concat(all_avg_list, ignore_index=True)
    df_lookup.set_index(['item', 'site', 'day_of_year', 'hour'], inplace=True)
    df_lookup.sort_index(inplace=True)
    return df_lookup

def init_worker(shared_df):
    global global_avg_lookup
    global_avg_lookup = shared_df

# ===============================================================
#   STEP 2 & 3: æ ¸å¿ƒè™•ç†èˆ‡ç¹ªåœ–
# ===============================================================

def process_and_plot(file_path):
    global global_avg_lookup
    file_name = os.path.splitext(os.path.basename(file_path))[0]

    report_list = []
    major_events = []

    try:
        # 1. è®€å–åŸå§‹è³‡æ–™
        df = pd.read_csv(file_path, dtype={'value': 'object'})
        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')

        if df['datetime'].dropna().empty: return None

        min_dt = df['datetime'].min()
        max_dt = df['datetime'].max()
        full_range = pd.date_range(min_dt, max_dt, freq='h')

        # 2. åš´æ ¼æª¢æŸ¥ CSV ç©ºå€¼
        null_mask = df['value'].isna()
        if null_mask.any():
            null_rows = df[null_mask]
            null_summary = null_rows.groupby(['site', 'item']).size().reset_index(name='count')
            for _, row in null_summary.iterrows():
                report_list.append({
                    'file': file_name, 'site': row['site'], 'item': row['item'],
                    'type': 'åŸå§‹è³‡æ–™ç©ºå€¼', 'detail': f"æœ‰ {row['count']} ç­†è¨˜éŒ„å€¼ç‚ºç©ºç™½"
                })

        # 3. æ•¸å€¼è½‰æ›èˆ‡éé æœŸæ–‡å­—æª¢æŸ¥
        df['numeric_value'] = pd.to_numeric(df['value'], errors='coerce')

        invalid_text_mask = df['numeric_value'].isna() & df['value'].notna()
        if invalid_text_mask.any():
            bad_rows = df[invalid_text_mask]
            for _, row in bad_rows.iterrows():
                report_list.append({
                    'file': file_name, 'site': row['site'], 'item': row['item'],
                    'type': 'éé æœŸæ–‡å­—', 'detail': f"Value: {row['value']}"
                })

        df = df.dropna(subset=['datetime', 'numeric_value'])
        df['value'] = df['numeric_value'].astype('float32')
        df = df.drop(columns=['numeric_value'])

        # 4. æ•¸å€¼é‚è¼¯æª¢æŸ¥
        valid_df_list = []
        for item, group in df.groupby('item'):
            info = items_info.get(item)
            if info:
                out_of_bound = (group['value'] < info['min']) | (group['value'] > info['max'])
                if out_of_bound.any():
                    errs = group[out_of_bound]
                    for _, row in errs.head(5).iterrows():
                        report_list.append({
                            'file': file_name, 'site': row['site'], 'item': row['item'],
                            'type': 'æ•¸å€¼è¶Šç•Œ', 'detail': f"Value: {row['value']} (Limit: {info['min']}~{info['max']})"
                        })
                group = group[~out_of_bound]
            valid_df_list.append(group)

        if not valid_df_list: return None
        df_clean = pd.concat(valid_df_list)

        # 5. ç¼ºå¤±è³‡æ–™åµæ¸¬ (Missing Timestamps)
        expected_len = len(full_range)
        for (site, item), group in df_clean.groupby(['site', 'item']):
            if len(group) < expected_len:
                existing_dts = set(group['datetime'])
                missing_count = expected_len - len(existing_dts)

                report_list.append({
                    'file': file_name, 'site': site, 'item': item,
                    'type': 'æ™‚æ®µè³‡æ–™éºå¤±', 'detail': f"ç¼ºå¤± {missing_count} å°æ™‚"
                })

        # 6. é‡å¤§äº‹ä»¶åµæ¸¬ (å…¨ç¶²ç„¡è³‡æ–™)
        existing_times = df_clean['datetime'].unique()
        missing_times = set(full_range) - set(existing_times)
        for t in missing_times:
            major_events.append({
                'file': file_name, 'datetime': t, 'event': 'é‡å¤§äº‹ä»¶ï¼šå…¨ç¶²æ–·è¨Š'
            })

        # 7. è¨ˆç®—è·å¹³
        df_clean['day_of_year'] = df_clean['datetime'].dt.dayofyear.astype('int16')
        df_clean['hour'] = df_clean['datetime'].dt.hour.astype('int8')
        df_clean.set_index(['item', 'site', 'day_of_year', 'hour'], inplace=True)

        merged = df_clean.join(global_avg_lookup, how='inner')
        merged['anomaly'] = merged['value'] - merged['avg_value']
        merged = merged.reset_index()

        if merged.empty: return None

        # 8. å€åŸŸå¹³å‡è¨ˆç®—
        merged['region'] = merged['site'].map(site_to_region)
        valid_regions = merged.dropna(subset=['region'])

        region_avg = valid_regions.groupby(['datetime', 'item', 'region'])['anomaly'].mean().reset_index()
        region_avg['site'] = "AVG_" + region_avg['region']

        # 9. ç¹ªåœ–èˆ‡ CSV è¼¸å‡º
        save_dir = os.path.join(img_dir, file_name)
        if not os.path.exists(save_dir): os.makedirs(save_dir)

        # è¼¸å‡º Raw Anomaly CSV (æœ¬åœ°å­˜æª”ä¿ç•™)
        csv_out_path = os.path.join(anomaly_dir, f"anomaly_{file_name}.csv")
        merged[['datetime', 'site', 'item', 'anomaly']].to_csv(csv_out_path, index=False)

        plot_count = 0
        items = region_avg['item'].unique()

        for item in items:
            reg_data = region_avg[region_avg['item'] == item]
            if reg_data.empty: continue

            # Pivot: datetime x region
            pivot_reg = reg_data.pivot(index='datetime', columns='site', values='anomaly')
            pivot_reg = pivot_reg.reindex(full_range)

            # --- ç¹ªåœ– ---
            fig, ax = plt.subplots(figsize=(15, 8))
            region_colors = {'AVG_åŒ—': 'blue', 'AVG_ä¸­': 'green', 'AVG_å—': 'red', 'AVG_æ±': 'orange'}

            for col in pivot_reg.columns:
                series = pivot_reg[col]
                if not series.dropna().empty:
                    color = region_colors.get(col, 'black')
                    ax.plot(pivot_reg.index, series, color=color, linewidth=2.5, label=col.replace('AVG_', '')+"éƒ¨")

            item_dict = items_info.get(item, {'name': item, 'unit': ''})

            ax.xaxis.set_major_locator(mdates.AutoDateLocator())
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d\n%H:%M'))

            ax.axhline(0, color='black', linestyle='--', alpha=0.5)
            ax.set_title(f"{file_name} - {item_dict['name']} å€åŸŸå¹³å‡è·å¹³", fontsize=16)
            ax.set_ylabel(f"è·å¹³ ({item_dict['unit']})")
            ax.legend(loc='upper right')
            plt.xticks(rotation=0)

            plt.savefig(os.path.join(save_dir, f"{item}.png"), dpi=100)
            plt.close(fig)
            plot_count += 1

        # 10. è¼¸å‡ºå ±å‘Š
        if report_list:
            pd.DataFrame(report_list).to_csv(os.path.join(report_dir, f"report_{file_name}.csv"), index=False, encoding='utf-8-sig')
        if major_events:
            pd.DataFrame(major_events).to_csv(os.path.join(report_dir, f"major_event_{file_name}.csv"), index=False, encoding='utf-8-sig')

        del df, df_clean, merged, pivot_reg
        gc.collect()

        return plot_count

    except Exception as e:
        with open(os.path.join(report_dir, f"CRITICAL_ERROR_{file_name}.txt"), "w") as f:
            f.write(str(e))
        return 0

# ===============================================================
#   ä¸»æµç¨‹
# ===============================================================

def main():
    print("ğŸš€ å•Ÿå‹•å·¥å…· (v3_NoDB - ç„¡è³‡æ–™åº«ç´”æ·¨ç‰ˆ)")

    # ä¸å†éœ€è¦ Manager å’Œ DB Writer Process
    df_avg_lookup = load_and_transform_averages()
    if df_avg_lookup is None:
        print("âŒ ç„¡æ­·å²è³‡æ–™ï¼ŒçµæŸç¨‹åºã€‚")
        return

    files = sorted(glob.glob(os.path.join(base_dir, "hourly_*.csv")))

    total_plots = 0
    start_time = time.time()

    # Init åªéœ€è¦å‚³é DataFrameï¼Œä¸éœ€è¦ Queue
    with ProcessPoolExecutor(max_workers=N_WORKERS, initializer=init_worker, initargs=(df_avg_lookup,)) as executor:
        future_to_file = {executor.submit(process_and_plot, f): f for f in files}

        for future in tqdm(as_completed(future_to_file), total=len(files), desc="è™•ç†é€²åº¦", unit="file"):
            try:
                res = future.result()
                if res: total_plots += res
            except Exception as e:
                print(f"Worker Error: {e}")
            gc.collect()

    del df_avg_lookup
    gc.collect()

    print(f"\nâœ… å®Œæˆï¼è€—æ™‚: {time.time() - start_time:.2f}ç§’, ç”¢å‡º {total_plots} å¼µåœ–è¡¨ã€‚")

if __name__ == '__main__':
    main()
