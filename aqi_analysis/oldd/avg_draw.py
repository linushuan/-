#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è·å¹³å€¼è¨ˆç®—èˆ‡ç¹ªåœ–å·¥å…· (PC ç‰ˆæœ¬)
åŠŸèƒ½ï¼š
 1. è¨ˆç®—å„æ¸¬é …çš„è·å¹³å€¼ (å¯¦éš›å€¼ - æ­·å²å¹³å‡å€¼)
 2. ç¹ªè£½è·å¹³å€¼æ™‚é–“åºåˆ—åœ–
 3. æ”¯æ´å€åŸŸå¹³å‡è·å¹³å€¼è¨ˆç®—
"""

import os
import glob
import matplotlib as mpl
mpl.use('Agg')
mpl.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.font_manager import fontManager, FontProperties
import pandas as pd
import numpy as np
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# ---------- Config ----------
current_dir = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.join(current_dir, "data")
output_dir = os.path.join(current_dir, "output_anomaly_pictures")
anomaly_dir = os.path.join(current_dir, "anomaly_csvs")
os.makedirs(output_dir, exist_ok=True)
os.makedirs(anomaly_dir, exist_ok=True)

# å€åŸŸå®šç¾©
areas = {
    "åŒ—": ["ä¸‰é‡", "ä¸­å£¢", "ä¸­å±±", "å†¬å±±", "å¤äº­", "åœŸåŸ", "åŸºéš†", "å£«æ—",
           "å¤§åœ’", "å®œè˜­", "å¹³é®", "æ–°åº—", "æ–°ç«¹", "æ–°èŠ", "æ¾å±±", "æ¿æ©‹",
           "æ—å£", "æ¡ƒåœ’", "æ°¸å’Œ", "æ±æ­¢", "æ·¡æ°´", "æ¹–å£", "ç«¹æ±", "èœå¯®",
           "è¬è¯", "è¬é‡Œ", "è§€éŸ³", "é™½æ˜", "é¾æ½­"],
    "ä¸­": ["ä¸‰ç¾©", "äºŒæ—", "å—æŠ•", "å¤§é‡Œ", "å½°åŒ–", "å¿ æ˜", "æ²™é¹¿",
           "ç·šè¥¿", "è‹—æ —", "è¥¿å±¯", "è±åŸ", "é ­ä»½"],
    "å—": ["ä»æ­¦", "å‰é‡‘", "å‰é®", "å–„åŒ–", "å˜‰ç¾©", "å¤§å¯®", "å®‰å—",
           "å°æ¸¯", "å±æ±", "å´™èƒŒ", "å·¦ç‡Ÿ", "å¾©èˆˆ", "æ†æ˜¥", "æ–—å…­",
           "æ–°æ¸¯", "æ–°ç‡Ÿ", "æœ´å­", "æ—åœ’", "æ¥ æ¢“", "æ©‹é ­", "æ½®å·",
           "ç¾æ¿ƒ", "è‡ºå—", "è‡ºè¥¿", "é³³å±±"],
    "æ±": ["è‡ºæ±", "èŠ±è“®"]
}

# æ¸¬é …è³‡è¨Š
items_info = {
    "AMB_TEMP": {"name": "ç’°å¢ƒæº«åº¦", "unit": "Â°C", "color": "#FF6B6B"},
    "CO": {"name": "ä¸€æ°§åŒ–ç¢³", "unit": "ppm", "color": "#4ECDC4"},
    "NO": {"name": "ä¸€æ°§åŒ–æ°®", "unit": "ppb", "color": "#45B7D1"},
    "NO2": {"name": "äºŒæ°§åŒ–æ°®", "unit": "ppb", "color": "#96CEB4"},
    "NOx": {"name": "æ°®æ°§åŒ–ç‰©", "unit": "ppb", "color": "#FFEAA7"},
    "O3": {"name": "è‡­æ°§", "unit": "ppb", "color": "#DBA3EA"},
    "PM10": {"name": "PM10", "unit": "Î¼g/mÂ³", "color": "#A08DFF"},
    "PM2.5": {"name": "PM2.5", "unit": "Î¼g/mÂ³", "color": "#FD79A8"},
    "RAINFALL": {"name": "é™é›¨é‡", "unit": "mm", "color": "#74B9FF"},
    "RH": {"name": "ç›¸å°æ¿•åº¦", "unit": "%", "color": "#81ECEC"},
    "SO2": {"name": "äºŒæ°§åŒ–ç¡«", "unit": "ppb", "color": "#FAB1A0"},
    "WD_HR": {"name": "å¹³å‡é¢¨å‘", "unit": "degrees", "color": "#00B894"},
    "WIND_DIREC": {"name": "é¢¨å‘", "unit": "degrees", "color": "#00CEC9"},
    "WIND_SPEED": {"name": "é¢¨é€Ÿ", "unit": "m/s", "color": "#0984E3"},
    "WS_HR": {"name": "å¹³å‡é¢¨é€Ÿ", "unit": "m/s", "color": "#6C5CE7"}
}

# ---------- å­—é«”è¨­å®š ----------
def set_chinese_font():
    p = "/usr/share/fonts/noto-cjk/NotoSansCJK-Regular.ttc"
    try:
        fontManager.addfont(p)
        prop = FontProperties(fname=p)
        found_name = prop.get_name()
        if found_name:
            mpl.rcParams['font.family'] = 'sans-serif'
            mpl.rcParams['font.sans-serif'] = [found_name]
            print(f"âœ… å­—é«”è¨­å®šå®Œæˆ: {found_name}")
        return found_name
    except Exception as e:
        print(f"âš ï¸ å­—é«”è¼‰å…¥å¤±æ•—: {e}")
        return None

set_chinese_font()

# ===============================================================
#   STEP 1: è¨ˆç®—è·å¹³å€¼
# ===============================================================

def load_historical_averages(item):
    """è¼‰å…¥æ­·å²å¹³å‡å€¼"""
    avg_file = os.path.join(base_dir, f"{item.lower()}_hourly_avg_fast.csv")
    
    if not os.path.exists(avg_file):
        print(f"âš ï¸ æ‰¾ä¸åˆ°å¹³å‡å€¼æª”æ¡ˆ: {avg_file}")
        return None
    
    try:
        df_avg = pd.read_csv(avg_file, encoding='utf-8-sig')
    except:
        try:
            df_avg = pd.read_csv(avg_file, encoding='utf-8')
        except Exception as e:
            print(f"âŒ è®€å–å¹³å‡å€¼æª”æ¡ˆå¤±æ•—: {e}")
            return None
    
    return df_avg


def calculate_anomalies_for_file(file_path, df_avg_dict):
    """
    è¨ˆç®—å–®ä¸€æª”æ¡ˆæ‰€æœ‰æ¸¬é …çš„è·å¹³å€¼
    
    Parameters:
    -----------
    file_path : str
        hourly CSV æª”æ¡ˆè·¯å¾‘
    df_avg_dict : dict
        {item: df_avg} çš„å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰æ¸¬é …çš„æ­·å²å¹³å‡å€¼
    
    Returns:
    --------
    DataFrame with anomalies for all items
    """
    try:
        df = pd.read_csv(file_path)
        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
        df = df.dropna(subset=['datetime'])
        
        # è¨ˆç®— day_of_year å’Œ hour
        df['day_of_year'] = df['datetime'].dt.dayofyear
        df['hour'] = df['datetime'].dt.hour
        
        all_anomalies = []
        
        # å°æ¯å€‹æ¸¬é …è¨ˆç®—è·å¹³å€¼
        for item in df['item'].unique():
            if item not in df_avg_dict:
                continue
            
            df_avg = df_avg_dict[item]
            df_item = df[df['item'] == item].copy()
            
            # å°æ¯å€‹ç«™é»è¨ˆç®—è·å¹³å€¼
            for site in df_item['site'].unique():
                site_data = df_item[df_item['site'] == site].copy()
                
                # æ‰¾åˆ°è©²ç«™é»åœ¨å¹³å‡å€¼è¡¨ä¸­çš„è³‡æ–™
                if 'æ¸¬ç«™' in df_avg.columns:
                    avg_row = df_avg[df_avg['æ¸¬ç«™'] == site]
                elif 'site' in df_avg.columns:
                    avg_row = df_avg[df_avg['site'] == site]
                else:
                    continue
                
                if avg_row.empty:
                    continue
                
                # æ‰¹æ¬¡è¨ˆç®—è·å¹³å€¼
                for idx, row in site_data.iterrows():
                    day = int(row['day_of_year'])
                    hour = int(row['hour'])
                    actual_value = row['value']
                    
                    col_name = f"{day}_{hour}"
                    
                    if col_name in avg_row.columns:
                        avg_value = avg_row[col_name].values[0]
                        
                        if pd.notna(avg_value) and pd.notna(actual_value):
                            anomaly = actual_value - avg_value
                            all_anomalies.append({
                                'datetime': row['datetime'],
                                'site': site,
                                'item': item,
                                'actual_value': actual_value,
                                'avg_value': avg_value,
                                'anomaly': anomaly
                            })
        
        if not all_anomalies:
            return None
        
        return pd.DataFrame(all_anomalies)
        
    except Exception as e:
        print(f"âŒ è™•ç†æª”æ¡ˆå¤±æ•— {file_path}: {e}")
        return None


def calculate_all_anomalies(file_pattern='hourly_*.csv'):
    """
    è¨ˆç®—æ‰€æœ‰æª”æ¡ˆçš„è·å¹³å€¼ä¸¦å„²å­˜
    """
    print("\n" + "="*60)
    print("  ğŸ“Š é–‹å§‹è¨ˆç®—è·å¹³å€¼")
    print("="*60)
    
    # è¼‰å…¥æ‰€æœ‰æ¸¬é …çš„æ­·å²å¹³å‡å€¼
    print("\nğŸ”„ è¼‰å…¥æ­·å²å¹³å‡å€¼...")
    df_avg_dict = {}
    for item in items_info.keys():
        df_avg = load_historical_averages(item)
        if df_avg is not None:
            df_avg_dict[item] = df_avg
            print(f"  âœ… {item}")
    
    if not df_avg_dict:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ­·å²å¹³å‡å€¼æª”æ¡ˆï¼")
        return
    
    # æ‰¾å‡ºæ‰€æœ‰ hourly æª”æ¡ˆ
    pattern = os.path.join(base_dir, file_pattern)
    files = sorted(glob.glob(pattern))
    
    if not files:
        print(f"âŒ æ‰¾ä¸åˆ°ç¬¦åˆæ¢ä»¶çš„æª”æ¡ˆ: {pattern}")
        return
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ")
    
    # å„²å­˜æ‰€æœ‰æ¸¬é …çš„è·å¹³å€¼
    all_item_anomalies = {item: [] for item in items_info.keys()}
    
    # è™•ç†æ¯å€‹æª”æ¡ˆ
    for file_path in tqdm(files, desc="è¨ˆç®—è·å¹³å€¼", unit="file"):
        df_anomalies = calculate_anomalies_for_file(file_path, df_avg_dict)
        
        if df_anomalies is not None and not df_anomalies.empty:
            # æŒ‰æ¸¬é …åˆ†é¡
            for item in df_anomalies['item'].unique():
                item_data = df_anomalies[df_anomalies['item'] == item]
                all_item_anomalies[item].append(item_data)
    
    # åˆä½µä¸¦å„²å­˜æ¯å€‹æ¸¬é …çš„è·å¹³å€¼
    print("\nğŸ’¾ å„²å­˜è·å¹³å€¼ CSV...")
    for item in items_info.keys():
        if all_item_anomalies[item]:
            df_combined = pd.concat(all_item_anomalies[item], ignore_index=True)
            df_combined = df_combined.sort_values('datetime')
            
            output_file = os.path.join(anomaly_dir, f"anomaly_{item}.csv")
            df_combined.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"  âœ… {item}: {len(df_combined)} ç­†è¨˜éŒ„")
    
    print("\nğŸ‰ è·å¹³å€¼è¨ˆç®—å®Œæˆï¼")


# ===============================================================
#   STEP 2: ç¹ªè£½è·å¹³å€¼åœ–è¡¨
# ===============================================================

def plot_anomaly_task(task):
    """
    ç¹ªè£½å–®ä¸€ç«™é»æˆ–å€åŸŸçš„è·å¹³å€¼åœ–
    """
    try:
        anomaly_df = task['data']
        item = task['item']
        site = task['site']
        info = task['info']
        output_path = task['output_path']
        is_region = task.get('is_region', False)
        
        if anomaly_df.empty:
            return None
        
        fig, ax = plt.subplots(figsize=(16, 8))
        
        # ç¹ªè£½è·å¹³å€¼ï¼ˆä½¿ç”¨é¡è‰²å€åˆ†æ­£è² å€¼ï¼‰
        positive = anomaly_df[anomaly_df['anomaly'] >= 0]
        negative = anomaly_df[anomaly_df['anomaly'] < 0]
        
        if not positive.empty:
            ax.scatter(positive['datetime'], positive['anomaly'], 
                      c='red', alpha=0.6, s=30, label='æ­£è·å¹³', zorder=5)
        if not negative.empty:
            ax.scatter(negative['datetime'], negative['anomaly'], 
                      c='blue', alpha=0.6, s=30, label='è² è·å¹³', zorder=5)
        
        # æ·»åŠ é›¶ç·š
        ax.axhline(y=0, color='gray', linestyle='--', linewidth=1.5, alpha=0.7, zorder=1)
        
        # è¨­å®šæ¨™é¡Œ
        title_prefix = "å€åŸŸå¹³å‡" if is_region else "æ¸¬ç«™"
        time_range = f"{anomaly_df['datetime'].min().strftime('%Y-%m-%d')} ~ {anomaly_df['datetime'].max().strftime('%Y-%m-%d')}"
        ax.set_title(f"{title_prefix} {site} - {info['name']} è·å¹³å€¼\n{time_range}",
                    fontsize=14, fontweight='bold', pad=15)
        ax.set_xlabel('æ™‚é–“', fontsize=12)
        ax.set_ylabel(f"è·å¹³å€¼ ({info['unit']})", fontsize=12)
        
        # æ ¼å¼åŒ– x è»¸
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d\n%H:%M'))
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())
        plt.setp(ax.get_xticklabels(), rotation=30, ha='right')
        
        # ç¶²æ ¼ç·šå’Œåœ–ä¾‹
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.legend(loc='best', fontsize=11, frameon=True, shadow=True)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close(fig)
        
        return output_path
        
    except Exception as e:
        print(f"âŒ ç¹ªåœ–éŒ¯èª¤: {e}")
        if 'fig' in locals():
            plt.close(fig)
        return None


def calculate_regional_anomalies(df_anomaly):
    """
    è¨ˆç®—å€åŸŸå¹³å‡è·å¹³å€¼
    """
    regional_data = []
    
    for region_name, region_sites in areas.items():
        # éæ¿¾è©²å€åŸŸçš„ç«™é»
        region_df = df_anomaly[df_anomaly['site'].isin(region_sites)].copy()
        
        if region_df.empty:
            continue
        
        # è¨ˆç®—å€åŸŸå¹³å‡è·å¹³å€¼
        regional_avg = (region_df.groupby(['datetime', 'item'], as_index=False)
                       .agg({
                           'anomaly': 'mean',
                           'actual_value': 'mean',
                           'avg_value': 'mean'
                       }))
        regional_avg['site'] = f"AVG_{region_name}"
        regional_data.append(regional_avg)
    
    if regional_data:
        return pd.concat(regional_data, ignore_index=True)
    return pd.DataFrame()


def plot_anomalies_for_item(item, include_regions=True, n_workers=4):
    """
    ç¹ªè£½å–®ä¸€æ¸¬é …çš„æ‰€æœ‰è·å¹³å€¼åœ–è¡¨
    """
    anomaly_file = os.path.join(anomaly_dir, f"anomaly_{item}.csv")
    
    if not os.path.exists(anomaly_file):
        print(f"âš ï¸ æ‰¾ä¸åˆ°è·å¹³å€¼æª”æ¡ˆ: {anomaly_file}")
        return 0, 0
    
    try:
        df = pd.read_csv(anomaly_file)
        df['datetime'] = pd.to_datetime(df['datetime'])
    except Exception as e:
        print(f"âŒ è®€å–æª”æ¡ˆå¤±æ•—: {e}")
        return 0, 0
    
    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
    item_output_dir = os.path.join(output_dir, f"anomaly_{item}")
    os.makedirs(item_output_dir, exist_ok=True)
    
    tasks = []
    info = items_info.get(item, {"name": item, "unit": "", "color": "#95A5A6"})
    
    # ç‚ºæ¯å€‹ç«™é»æº–å‚™ç¹ªåœ–ä»»å‹™
    for site in df['site'].unique():
        site_data = df[df['site'] == site].copy()
        
        if not site_data.empty:
            output_path = os.path.join(item_output_dir, f"{site}_anomaly.png")
            tasks.append({
                'data': site_data,
                'item': item,
                'site': site,
                'info': info,
                'output_path': output_path,
                'is_region': False
            })
    
    # è¨ˆç®—ä¸¦ç¹ªè£½å€åŸŸå¹³å‡
    if include_regions:
        df_regional = calculate_regional_anomalies(df)
        if not df_regional.empty:
            for region_site in df_regional['site'].unique():
                region_data = df_regional[df_regional['site'] == region_site].copy()
                
                if not region_data.empty:
                    output_path = os.path.join(item_output_dir, 
                                              f"{region_site}_anomaly.png")
                    tasks.append({
                        'data': region_data,
                        'item': item,
                        'site': region_site,
                        'info': info,
                        'output_path': output_path,
                        'is_region': True
                    })
    
    # å¹³è¡Œè™•ç†ç¹ªåœ–
    total = len(tasks)
    success = 0
    
    with ProcessPoolExecutor(max_workers=n_workers) as exc:
        futures = [exc.submit(plot_anomaly_task, t) for t in tasks]
        for f in tqdm(as_completed(futures), total=total, 
                     desc=f'  ç¹ªè£½ {item}', leave=False, unit='åœ–'):
            if f.result():
                success += 1
    
    return success, total


def plot_all_anomalies(include_regions=True, n_workers=4):
    """
    ç¹ªè£½æ‰€æœ‰æ¸¬é …çš„è·å¹³å€¼åœ–è¡¨
    """
    print("\n" + "="*60)
    print("  ğŸ¨ é–‹å§‹ç¹ªè£½è·å¹³å€¼åœ–è¡¨")
    print("="*60)
    
    total_success = 0
    total_tasks = 0
    
    for item in tqdm(items_info.keys(), desc="è™•ç†æ¸¬é …", unit="item"):
        success, total = plot_anomalies_for_item(item, include_regions, n_workers)
        total_success += success
        total_tasks += total
        if total > 0:
            tqdm.write(f"  âœ… {item}: {success}/{total} å¼µ")
    
    print(f"\nğŸ‰ ç¸½è¨ˆå®Œæˆ {total_success}/{total_tasks} å¼µåœ–")
    print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {output_dir}")


# ===============================================================
#   ä¸»ç¨‹å¼
# ===============================================================

if __name__ == '__main__':
    print("\n" + "="*60)
    print("  ğŸŒ¡ï¸  è·å¹³å€¼è¨ˆç®—èˆ‡ç¹ªåœ–å·¥å…· (PC ç‰ˆæœ¬)")
    print("="*60)
    
    # è¨­å®šåƒæ•¸
    FILE_PATTERN = 'hourly_2019*.csv'  # å¯èª¿æ•´è¦è™•ç†çš„æª”æ¡ˆç¯„åœ
    INCLUDE_REGIONS = True              # æ˜¯å¦åŒ…å«å€åŸŸå¹³å‡
    N_WORKERS = 10                  # å¹³è¡Œè™•ç†æ•¸é‡
    
    # Step 1: è¨ˆç®—è·å¹³å€¼
    calculate_all_anomalies(file_pattern=FILE_PATTERN)
    
    # Step 2: ç¹ªè£½è·å¹³å€¼åœ–è¡¨
    plot_all_anomalies(include_regions=INCLUDE_REGIONS, n_workers=N_WORKERS)
    
    print("\n" + "="*60)
    print("  âœ¨ æ‰€æœ‰ä»»å‹™å®Œæˆï¼")
    print(f"  ğŸ“ è·å¹³å€¼ CSV: {anomaly_dir}")
    print(f"  ğŸ“ è·å¹³å€¼åœ–è¡¨: {output_dir}")
    print("="*60 + "\n")
