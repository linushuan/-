#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è·å¹³å€¼è¨ˆç®—èˆ‡ç¹ªåœ–å·¥å…· (PC ç‰ˆæœ¬) - æŒ‰æ—¥æœŸåˆ†æª”
åŠŸèƒ½ï¼š
 1. è¨ˆç®—å„æ¸¬é …çš„è·å¹³å€¼ (å¯¦éš›å€¼ - æ­·å²å¹³å‡å€¼)
 2. æŒ‰åŸå§‹æª”æ¡ˆæ—¥æœŸç¯„åœåˆ†åˆ¥å„²å­˜ CSV
 3. ç¹ªè£½è·å¹³å€¼æ™‚é–“åºåˆ—åœ–ï¼ˆèˆ‡åŸå§‹ v5 ç›¸åŒæ ¼å¼ï¼‰
 4. æ”¯æ´å€åŸŸå¹³å‡è·å¹³å€¼è¨ˆç®—
"""

import os
import glob
import matplotlib as mpl
mpl.use('Agg')
mpl.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.font_manager import fontManager, FontProperties
import pandas as pd
import numpy as np
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# ---------- Config ----------
current_dir = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.join(current_dir, "data")
output_dir = os.path.join(current_dir, "output_anomaly_pictures2")
anomaly_dir = os.path.join(current_dir, "anomaly_csvs2")
os.makedirs(output_dir, exist_ok=True)
os.makedirs(anomaly_dir, exist_ok=True)

# å€åŸŸå®šç¾©
areas = {
    "åŒ—": ["ä¸‰é‡", "ä¸­å£¢", "ä¸­å±±", "å†¬å±±", "å¤äº­", "åœŸåŸ", "åŸºéš†", "å£«æ—",
           "å¤§åœ’", "å®œè˜­", "å¹³é®", "æ–°åº—", "æ–°ç«¹", "æ–°èŠ", "æ¾å±±", "æ¿æ©‹",
           "æ—å£", "æ¡ƒåœ’", "æ°¸å’Œ", "æ±æ­¢", "æ·¡æ°´", "æ¹–å£", "ç«¹æ±", "èœå¯®",
           "è¬è¯", "è¬é‡Œ", "è§€éŸ³", "é™½æ˜", "é¾æ½­"],
    "ä¸­": ["ä¸‰ç¾©", "äºŒæ—", "å—æŠ•", "å¤§é‡Œ", "å½°åŒ–", "å¿ æ˜", "æ²™é¹¿",
           "ç·šè¥¿", "è‹—æ —", "è¥¿å±¯", "è±åŸ", "é ­ä»½"],
    "å—": ["ä»æ­¦", "å‰é‡‘", "å‰é®", "å–„åŒ–", "å˜‰ç¾©", "å¤§å¯®", "å®‰å—",
           "å°æ¸¯", "å±æ±", "å´™èƒŒ", "å·¦ç‡Ÿ", "å¾©èˆˆ", "æ†æ˜¥", "æ–—å…­",
           "æ–°æ¸¯", "æ–°ç‡Ÿ", "æœ´å­", "æ—åœ’", "æ¥ æ¢“", "æ©‹é ­", "æ½®å·",
           "ç¾æ¿ƒ", "è‡ºå—", "è‡ºè¥¿", "é³³å±±"],
    "æ±": ["è‡ºæ±", "èŠ±è“®"]
}

# æ¸¬é …è³‡è¨Š
items_info = {
    "AMB_TEMP": {"name": "ç’°å¢ƒæº«åº¦", "unit": "Â°C", "color": "#FF6B6B"},
    "CO": {"name": "ä¸€æ°§åŒ–ç¢³", "unit": "ppm", "color": "#4ECDC4"},
    "NO": {"name": "ä¸€æ°§åŒ–æ°®", "unit": "ppb", "color": "#45B7D1"},
    "NO2": {"name": "äºŒæ°§åŒ–æ°®", "unit": "ppb", "color": "#96CEB4"},
    "NOx": {"name": "æ°®æ°§åŒ–ç‰©", "unit": "ppb", "color": "#FFEAA7"},
    "O3": {"name": "è‡­æ°§", "unit": "ppb", "color": "#DBA3EA"},
    "PM10": {"name": "PM10", "unit": "Î¼g/mÂ³", "color": "#A08DFF"},
    "PM2.5": {"name": "PM2.5", "unit": "Î¼g/mÂ³", "color": "#FD79A8"},
    "RAINFALL": {"name": "é™é›¨é‡", "unit": "mm", "color": "#74B9FF"},
    "RH": {"name": "ç›¸å°æ¿•åº¦", "unit": "%", "color": "#81ECEC"},
    "SO2": {"name": "äºŒæ°§åŒ–ç¡«", "unit": "ppb", "color": "#FAB1A0"},
    "WD_HR": {"name": "å¹³å‡é¢¨å‘", "unit": "degrees", "color": "#00B894"},
    "WIND_DIREC": {"name": "é¢¨å‘", "unit": "degrees", "color": "#00CEC9"},
    "WIND_SPEED": {"name": "é¢¨é€Ÿ", "unit": "m/s", "color": "#0984E3"},
    "WS_HR": {"name": "å¹³å‡é¢¨é€Ÿ", "unit": "m/s", "color": "#6C5CE7"}
}

# ---------- å­—é«”è¨­å®š ----------
def set_chinese_font():
    p = "/usr/share/fonts/noto-cjk/NotoSansCJK-Regular.ttc"
    try:
        fontManager.addfont(p)
        prop = FontProperties(fname=p)
        found_name = prop.get_name()
        if found_name:
            mpl.rcParams['font.family'] = 'sans-serif'
            mpl.rcParams['font.sans-serif'] = [found_name]
            print(f"âœ… å­—é«”è¨­å®šå®Œæˆ: {found_name}")
        return found_name
    except Exception as e:
        print(f"âš ï¸ å­—é«”è¼‰å…¥å¤±æ•—: {e}")
        return None

set_chinese_font()

# ===============================================================
#   STEP 1: è¨ˆç®—è·å¹³å€¼ï¼ˆæŒ‰æ—¥æœŸåˆ†æª”ï¼‰
# ===============================================================

def load_historical_averages():
    """è¼‰å…¥æ‰€æœ‰æ¸¬é …çš„æ­·å²å¹³å‡å€¼"""
    df_avg_dict = {}
    for item in items_info.keys():
        avg_file = os.path.join(base_dir, f"{item.lower()}_hourly_avg_fast.csv")
        
        if not os.path.exists(avg_file):
            continue
        
        try:
            df_avg = pd.read_csv(avg_file, encoding='utf-8-sig')
        except:
            try:
                df_avg = pd.read_csv(avg_file, encoding='utf-8')
            except:
                continue
        
        df_avg_dict[item] = df_avg
    
    return df_avg_dict


def calculate_anomalies_for_file(file_path, df_avg_dict):
    """
    è¨ˆç®—å–®ä¸€æª”æ¡ˆæ‰€æœ‰æ¸¬é …çš„è·å¹³å€¼ï¼Œä¸¦æŒ‰åŸå§‹æª”æ¡ˆæ—¥æœŸç¯„åœå„²å­˜
    
    Returns:
    --------
    file_name: åŸå§‹æª”æ¡ˆåç¨±ï¼ˆç”¨æ–¼å»ºç«‹å°æ‡‰çš„è·å¹³æª”æ¡ˆï¼‰
    """
    try:
        # è®€å–åŸå§‹è³‡æ–™
        df = pd.read_csv(file_path)
        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
        df = df.dropna(subset=['datetime'])
        
        if df.empty:
            return None, None
        
        # å–å¾—åŸå§‹æª”æ¡ˆåç¨±
        file_name = os.path.splitext(os.path.basename(file_path))[0]
        
        # è¨ˆç®— day_of_year å’Œ hour
        df['day_of_year'] = df['datetime'].dt.dayofyear
        df['hour'] = df['datetime'].dt.hour
        
        all_anomalies = []
        
        # å°æ¯å€‹æ¸¬é …è¨ˆç®—è·å¹³å€¼
        for item in df['item'].unique():
            if item not in df_avg_dict:
                continue
            
            df_avg = df_avg_dict[item]
            df_item = df[df['item'] == item].copy()
            
            # å°æ¯å€‹ç«™é»è¨ˆç®—è·å¹³å€¼
            for site in df_item['site'].unique():
                site_data = df_item[df_item['site'] == site].copy()
                
                # æ‰¾åˆ°è©²ç«™é»åœ¨å¹³å‡å€¼è¡¨ä¸­çš„è³‡æ–™
                if 'æ¸¬ç«™' in df_avg.columns:
                    avg_row = df_avg[df_avg['æ¸¬ç«™'] == site]
                elif 'site' in df_avg.columns:
                    avg_row = df_avg[df_avg['site'] == site]
                else:
                    continue
                
                if avg_row.empty:
                    continue
                
                # æ‰¹æ¬¡è¨ˆç®—è·å¹³å€¼
                for idx, row in site_data.iterrows():
                    day = int(row['day_of_year'])
                    hour = int(row['hour'])
                    actual_value = row['value']
                    
                    col_name = f"{day}_{hour}"
                    
                    if col_name in avg_row.columns:
                        avg_value = avg_row[col_name].values[0]
                        
                        if pd.notna(avg_value) and pd.notna(actual_value):
                            anomaly = actual_value - avg_value
                            all_anomalies.append({
                                'datetime': row['datetime'],
                                'site': site,
                                'item': item,
                                'value': anomaly  # æ”¹ç”¨ value å„²å­˜è·å¹³å€¼ï¼Œä¿æŒæ ¼å¼ä¸€è‡´
                            })
        
        if not all_anomalies:
            return file_name, None
        
        df_anomalies = pd.DataFrame(all_anomalies)
        df_anomalies = df_anomalies.sort_values('datetime')
        
        # å„²å­˜ç‚ºèˆ‡åŸå§‹æª”æ¡ˆå°æ‡‰çš„è·å¹³æª”æ¡ˆ
        output_file = os.path.join(anomaly_dir, f"anomaly_{file_name}.csv")
        df_anomalies.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        return file_name, len(all_anomalies)
        
    except Exception as e:
        print(f"âŒ è™•ç†æª”æ¡ˆå¤±æ•— {file_path}: {e}")
        return None, None


def calculate_all_anomalies(file_pattern='hourly_*.csv'):
    """
    è¨ˆç®—æ‰€æœ‰æª”æ¡ˆçš„è·å¹³å€¼ä¸¦æŒ‰æ—¥æœŸåˆ†åˆ¥å„²å­˜
    """
    print("\n" + "="*60)
    print("  ğŸ“Š é–‹å§‹è¨ˆç®—è·å¹³å€¼ï¼ˆæŒ‰æ—¥æœŸåˆ†æª”ï¼‰")
    print("="*60)
    
    # è¼‰å…¥æ‰€æœ‰æ¸¬é …çš„æ­·å²å¹³å‡å€¼
    print("\nğŸ”„ è¼‰å…¥æ­·å²å¹³å‡å€¼...")
    df_avg_dict = load_historical_averages()
    
    if not df_avg_dict:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ­·å²å¹³å‡å€¼æª”æ¡ˆï¼")
        return
    
    print(f"  âœ… è¼‰å…¥äº† {len(df_avg_dict)} å€‹æ¸¬é …çš„æ­·å²å¹³å‡å€¼")
    
    # æ‰¾å‡ºæ‰€æœ‰ hourly æª”æ¡ˆ
    pattern = os.path.join(base_dir, file_pattern)
    files = sorted(glob.glob(pattern))
    
    if not files:
        print(f"âŒ æ‰¾ä¸åˆ°ç¬¦åˆæ¢ä»¶çš„æª”æ¡ˆ: {pattern}")
        return
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ")
    
    # è™•ç†æ¯å€‹æª”æ¡ˆ
    total_records = 0
    successful_files = 0
    
    for file_path in tqdm(files, desc="è¨ˆç®—è·å¹³å€¼", unit="file"):
        file_name, record_count = calculate_anomalies_for_file(file_path, df_avg_dict)
        
        if record_count is not None and record_count > 0:
            total_records += record_count
            successful_files += 1
            tqdm.write(f"  âœ… {file_name}: {record_count} ç­†")
    
    print(f"\nğŸ‰ æˆåŠŸè™•ç† {successful_files}/{len(files)} å€‹æª”æ¡ˆ")
    print(f"ğŸ“Š ç¸½è¨ˆ {total_records} ç­†è·å¹³å€¼è¨˜éŒ„")


# ===============================================================
#   STEP 2: ç¹ªè£½è·å¹³å€¼åœ–è¡¨ï¼ˆèˆ‡ v5 ç›¸åŒæ ¼å¼ï¼‰
# ===============================================================

def plot_overlay_task(task):
    """
    ç¹ªè£½ç–ŠåŠ åœ–ï¼ˆèˆ‡ v5 æ ¼å¼ç›¸åŒï¼‰
    """
    try:
        item = task['item']
        site_data = task['site_data']
        info = task['info']
        note = task.get('note', '')
        fig, ax = plt.subplots(figsize=task['figsize'])

        region_styles = {
            "AVG_åŒ—": {"color": "blue", "label": "åŒ—éƒ¨å¹³å‡"},
            "AVG_ä¸­": {"color": "green", "label": "ä¸­éƒ¨å¹³å‡"},
            "AVG_å—": {"color": "red", "label": "å—éƒ¨å¹³å‡"},
            "AVG_æ±": {"color": "orange", "label": "æ±éƒ¨å¹³å‡"}
        }

        # ç¹ªè£½å€‹åˆ¥ç«™é»ï¼ˆç°è‰²ç´°ç·šï¼‰
        for site_name, data in site_data.items():
            if site_name not in region_styles:
                dt_arr = pd.to_datetime(data['times']).to_pydatetime()
                val_arr = data['values']
                ax.plot(dt_arr, val_arr, lw=1.5, alpha=0.5, 
                       color='gray', zorder=5)
        
        # ç¹ªè£½å€åŸŸå¹³å‡ï¼ˆå½©è‰²ç²—ç·šï¼‰
        for site_name, data in site_data.items():
            if site_name in region_styles:
                style = region_styles[site_name]
                dt_arr = pd.to_datetime(data['times']).to_pydatetime()
                val_arr = data['values']
                ax.plot(dt_arr, val_arr, label=style['label'], 
                       color=style['color'], lw=3, ls='-', zorder=10)

        # æ·»åŠ é›¶ç·š
        ax.axhline(y=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7, zorder=1)

        ax.set_title(f"å€åŸŸåˆ†æ - {info.get('name', item)} è·å¹³å€¼ {note}", 
                    fontsize=14, fontweight='bold')
        ax.set_ylabel(f"{info.get('name', item)} è·å¹³å€¼ ({info.get('unit','')})")
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d\n%H:%M'))
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())
        plt.setp(ax.get_xticklabels(), rotation=30, ha='right')
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.legend(fontsize=10, frameon=True, shadow=True)
        plt.tight_layout()
        plt.savefig(task['output_path'], dpi=150, bbox_inches='tight')
        plt.close(fig)
        return task['output_path']
    except Exception as e:
        print(f"ç¹ªåœ–éŒ¯èª¤: {e}")
        if 'fig' in locals():
            plt.close(fig)
        return None


def process_anomaly_dataframe(df, regions_to_plot):
    """
    è™•ç†è·å¹³å€¼è³‡æ–™ï¼Œè¨ˆç®—å€åŸŸå¹³å‡
    """
    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
    df = df.dropna(subset=['datetime'])
    
    dfs_to_concat = [df]

    # è¨ˆç®—å€åŸŸå¹³å‡
    region_dfs = []
    for region_name, region_sites in areas.items():
        avg_site_name = f"AVG_{region_name}"
        
        if regions_to_plot and avg_site_name not in regions_to_plot:
            continue
            
        mask = df['site'].isin(region_sites)
        if not mask.any():
            continue
        
        mean_df = (df[mask]
                  .groupby(['datetime', 'item'], as_index=False)
                  ['value'].mean(numeric_only=True))
        mean_df['site'] = avg_site_name
        region_dfs.append(mean_df)
    
    if region_dfs:
        dfs_to_concat.extend(region_dfs)

    df_result = pd.concat(dfs_to_concat, ignore_index=True)
    df_result = df_result.sort_values('datetime')
    
    return df_result


def prepare_plot_tasks(df, file_name, regions_to_plot):
    """
    æº–å‚™ç¹ªåœ–ä»»å‹™
    """
    tasks = []
    file_output_dir = os.path.join(output_dir, file_name)
    os.makedirs(file_output_dir, exist_ok=True)

    for item, group in df.groupby('item'):
        site_data = {
            site: {
                'times': sub['datetime'].values, 
                'values': sub['value'].values
            }
            for site, sub in group.groupby('site')
        }
        
        # å»ºç«‹è¼¸å‡ºæª”å
        region_str = '_'.join([r.replace('AVG_', '') for r in regions_to_plot])
        output_filename = f"ANOMALY_{item}_{region_str}.png"
        
        tasks.append({
            'item': item,
            'site_data': site_data,
            'output_path': os.path.join(file_output_dir, output_filename),
            'figsize': (16, 8),
            'info': items_info.get(item, {'name': item, 'unit': ''}),
            'note': ''
        })
    return tasks


def plot_anomaly_file(file_path, regions_to_plot, n_workers):
    """
    ç¹ªè£½å–®ä¸€è·å¹³æª”æ¡ˆçš„åœ–è¡¨
    """
    try:
        df = pd.read_csv(file_path)
        file_name = os.path.splitext(os.path.basename(file_path))[0]
        
        # è™•ç†è³‡æ–™ï¼ˆè¨ˆç®—å€åŸŸå¹³å‡ï¼‰
        df_proc = process_anomaly_dataframe(df, regions_to_plot)
        
        if df_proc.empty:
            return 0, 0
        
        # æº–å‚™ç¹ªåœ–ä»»å‹™
        tasks = prepare_plot_tasks(df_proc, file_name, regions_to_plot)
        
        total = len(tasks)
        success = 0
        
        with ProcessPoolExecutor(max_workers=n_workers) as exc:
            futures = [exc.submit(plot_overlay_task, t) for t in tasks]
            for f in as_completed(futures):
                if f.result():
                    success += 1
        
        return success, total
        
    except Exception as e:
        print(f"âŒ è™•ç†æª”æ¡ˆå¤±æ•—: {e}")
        return 0, 0


def plot_all_anomalies(regions_to_plot, n_workers=4, file_pattern='anomaly_hourly_*.csv'):
    """
    ç¹ªè£½æ‰€æœ‰è·å¹³å€¼åœ–è¡¨
    """
    print("\n" + "="*60)
    print("  ğŸ¨ é–‹å§‹ç¹ªè£½è·å¹³å€¼åœ–è¡¨")
    print("="*60)
    
    # æ‰¾å‡ºæ‰€æœ‰è·å¹³æª”æ¡ˆ
    pattern = os.path.join(anomaly_dir, file_pattern)
    files = sorted(glob.glob(pattern))
    
    if not files:
        print(f"âŒ æ‰¾ä¸åˆ°è·å¹³æª”æ¡ˆ: {pattern}")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(files)} å€‹è·å¹³æª”æ¡ˆ")
    print(f"ğŸŒ ç¹ªè£½å€åŸŸ: {', '.join(regions_to_plot)}")
    
    total_success = 0
    total_tasks = 0
    
    for file_path in tqdm(files, desc="ç¹ªè£½åœ–è¡¨", unit="file"):
        file_name = os.path.basename(file_path)
        success, total = plot_anomaly_file(file_path, regions_to_plot, n_workers)
        total_success += success
        total_tasks += total
        if total > 0:
            tqdm.write(f"  âœ… {file_name}: {success}/{total} å¼µ")
    
    print(f"\nğŸ‰ ç¸½è¨ˆå®Œæˆ {total_success}/{total_tasks} å¼µåœ–")
    print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {output_dir}")


# ===============================================================
#   ä¸»ç¨‹å¼
# ===============================================================

if __name__ == '__main__':
    print("\n" + "="*60)
    print("  ğŸŒ¡ï¸  è·å¹³å€¼è¨ˆç®—èˆ‡ç¹ªåœ–å·¥å…· (PC ç‰ˆæœ¬)")
    print("="*60)
    
    # è¨­å®šåƒæ•¸
    FILE_PATTERN = 'hourly_2019*.csv'           # è¦è™•ç†çš„æª”æ¡ˆç¯„åœ
    REGIONS = ['AVG_å—', 'AVG_åŒ—', 'AVG_ä¸­', 'AVG_æ±']  # è¦ç¹ªè£½çš„å€åŸŸ
    N_WORKERS = 10                                # å¹³è¡Œè™•ç†æ•¸é‡
    
    # Step 1: è¨ˆç®—è·å¹³å€¼ï¼ˆæŒ‰æ—¥æœŸåˆ†æª”ï¼‰
    calculate_all_anomalies(file_pattern=FILE_PATTERN)
    
    # Step 2: ç¹ªè£½è·å¹³å€¼åœ–è¡¨
    plot_all_anomalies(regions_to_plot=REGIONS, n_workers=N_WORKERS)
    
    print("\n" + "="*60)
    print("  âœ¨ æ‰€æœ‰ä»»å‹™å®Œæˆï¼")
    print(f"  ğŸ“ è·å¹³å€¼ CSV: {anomaly_dir}")
    print(f"  ğŸ“ è·å¹³å€¼åœ–è¡¨: {output_dir}")
    print("="*60 + "\n")
