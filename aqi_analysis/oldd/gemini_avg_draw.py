#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é«˜æ•ˆèƒ½è·å¹³å€¼è¨ˆç®—èˆ‡ç¹ªåœ–å·¥å…· (æœ€çµ‚ç‰ˆ)
ç‰¹è‰²ï¼š
 1. ç¨ç«‹å ±å‘Šï¼šæ¯å€‹æª”æ¡ˆç¼ºå¤±æƒ…å½¢ç¨ç«‹è¼¸å‡º CSVã€‚
 2. æ–·ç·šç¹ªåœ–ï¼šç¼ºå¤±è³‡æ–™è™•è‡ªå‹•æ–·é–‹ä¸é€£ç·šã€‚
 3. å€åŸŸå¹³å‡ï¼šéƒ¨åˆ†ç¼ºå¤±ä»è¨ˆç®—å¹³å‡ï¼Œå…¨ç¼ºå¤±å‰‡ç‚ºç©ºã€‚
 4. é€²åº¦æ¢ï¼šæ¢å¾©è©³ç´°é¡¯ç¤ºæ¨¡å¼ (é¡¯ç¤ºä»»å‹™åç¨±èˆ‡æª”æ¡ˆå–®ä½)ã€‚
"""

import os
import glob
import time
import gc
import pandas as pd
import numpy as np
import matplotlib as mpl
mpl.use('Agg')
mpl.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.font_manager import fontManager, FontProperties
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# ===============================================================
#   CONFIG è¨­å®šå€
# ===============================================================

N_WORKERS = 4  # ç¶­æŒç©©å®šè¨­å®š

# è·¯å¾‘è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.join(current_dir, "data")
output_root = os.path.join(current_dir, "output_results")
anomaly_dir = os.path.join(output_root, "anomaly_csvs")
img_dir = os.path.join(output_root, "anomaly_pictures")
report_dir = os.path.join(output_root, "reports")

for d in [base_dir, anomaly_dir, img_dir, report_dir]:
    os.makedirs(d, exist_ok=True)

# å€åŸŸå®šç¾©
areas = {
    "åŒ—": ["ä¸‰é‡", "ä¸­å£¢", "ä¸­å±±", "å†¬å±±", "å¤äº­", "åœŸåŸ", "åŸºéš†", "å£«æ—",
           "å¤§åœ’", "å®œè˜­", "å¹³é®", "æ–°åº—", "æ–°ç«¹", "æ–°èŠ", "æ¾å±±", "æ¿æ©‹",
           "æ—å£", "æ¡ƒåœ’", "æ°¸å’Œ", "æ±æ­¢", "æ·¡æ°´", "æ¹–å£", "ç«¹æ±", "èœå¯®",
           "è¬è¯", "è¬é‡Œ", "è§€éŸ³", "é™½æ˜", "é¾æ½­"],
    "ä¸­": ["ä¸‰ç¾©", "äºŒæ—", "å—æŠ•", "å¤§é‡Œ", "å½°åŒ–", "å¿ æ˜", "æ²™é¹¿",
           "ç·šè¥¿", "è‹—æ —", "è¥¿å±¯", "è±åŸ", "é ­ä»½"],
    "å—": ["ä»æ­¦", "å‰é‡‘", "å‰é®", "å–„åŒ–", "å˜‰ç¾©", "å¤§å¯®", "å®‰å—",
           "å°æ¸¯", "å±æ±", "å´™èƒŒ", "å·¦ç‡Ÿ", "å¾©èˆˆ", "æ†æ˜¥", "æ–—å…­",
           "æ–°æ¸¯", "æ–°ç‡Ÿ", "æœ´å­", "æ—åœ’", "æ¥ æ¢“", "æ©‹é ­", "æ½®å·",
           "ç¾æ¿ƒ", "è‡ºå—", "è‡ºè¥¿", "é³³å±±"],
    "æ±": ["è‡ºæ±", "èŠ±è“®"]
}

site_to_region = {}
for region, sites in areas.items():
    for site in sites:
        site_to_region[site] = region

items_info = {
    "AMB_TEMP": {"name": "ç’°å¢ƒæº«åº¦", "unit": "Â°C"},
    "CO": {"name": "ä¸€æ°§åŒ–ç¢³", "unit": "ppm"},
    "NO": {"name": "ä¸€æ°§åŒ–æ°®", "unit": "ppb"},
    "NO2": {"name": "äºŒæ°§åŒ–æ°®", "unit": "ppb"},
    "NOx": {"name": "æ°®æ°§åŒ–ç‰©", "unit": "ppb"},
    "O3": {"name": "è‡­æ°§", "unit": "ppb"},
    "PM10": {"name": "PM10", "unit": "Î¼g/mÂ³"},
    "PM2.5": {"name": "PM2.5", "unit": "Î¼g/mÂ³"},
    "RAINFALL": {"name": "é™é›¨é‡", "unit": "mm"},
    "RH": {"name": "ç›¸å°æ¿•åº¦", "unit": "%"},
    "SO2": {"name": "äºŒæ°§åŒ–ç¡«", "unit": "ppb"},
    "WD_HR": {"name": "å¹³å‡é¢¨å‘", "unit": "degrees"},
    "WIND_DIREC": {"name": "é¢¨å‘", "unit": "degrees"},
    "WIND_SPEED": {"name": "é¢¨é€Ÿ", "unit": "m/s"},
    "WS_HR": {"name": "å¹³å‡é¢¨é€Ÿ", "unit": "m/s"}
}

def set_chinese_font():
    p = "/usr/share/fonts/noto-cjk/NotoSansCJK-Regular.ttc"
    try:
        fontManager.addfont(p)
        prop = FontProperties(fname=p)
        if prop.get_name():
            mpl.rcParams['font.family'] = 'sans-serif'
            mpl.rcParams['font.sans-serif'] = [prop.get_name()]
    except Exception:
        pass
set_chinese_font()

# ===============================================================
#   STEP 1: é è™•ç†æ­·å²å¹³å‡å€¼ (Global Shared)
# ===============================================================

global_avg_lookup = None

def load_and_transform_averages():
    print("ğŸ”„ è¼‰å…¥æ­·å²å¹³å‡å€¼...")
    all_avg_list = []
    for item in items_info.keys():
        avg_file = os.path.join(base_dir, f"{item.lower()}_hourly_avg_fast.csv")
        if not os.path.exists(avg_file): continue
        try:
            df = pd.read_csv(avg_file)
            if 'æ¸¬ç«™' in df.columns: df = df.rename(columns={'æ¸¬ç«™': 'site'})
            if 'site' not in df.columns: continue

            df_melted = df.melt(id_vars=['site'], var_name='day_hour', value_name='avg_value')
            temp_split = df_melted['day_hour'].str.split('_', expand=True)

            df_melted['day_of_year'] = temp_split[0].astype('int16')
            df_melted['hour'] = temp_split[1].astype('int8')
            df_melted['item'] = item
            df_melted['avg_value'] = df_melted['avg_value'].astype('float32')

            df_melted = df_melted.drop(columns=['day_hour']).dropna(subset=['avg_value'])
            all_avg_list.append(df_melted)
            del df, temp_split
        except Exception:
            pass

    if not all_avg_list: return None
    print("âš¡ åˆä½µç´¢å¼•ä¸­...")
    df_lookup = pd.concat(all_avg_list, ignore_index=True)
    df_lookup['site'] = df_lookup['site'].astype(str)
    df_lookup['item'] = df_lookup['item'].astype(str)

    del all_avg_list
    gc.collect()
    return df_lookup

def init_worker(shared_df):
    global global_avg_lookup
    global_avg_lookup = shared_df

# ===============================================================
#   STEP 2: æ ¸å¿ƒè™•ç†
# ===============================================================

def process_single_file(file_path):
    global global_avg_lookup
    file_name = os.path.splitext(os.path.basename(file_path))[0]
    missing_report = []

    try:
        # 1. è®€å–
        df = pd.read_csv(file_path, usecols=['datetime', 'site', 'item', 'value'])
        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
        df = df.dropna(subset=['datetime'])
        if df.empty: return None

        df['site'] = df['site'].astype(str)
        df['item'] = df['item'].astype(str)
        df['value'] = pd.to_numeric(df['value'], errors='coerce').astype('float32')

        # --- 2. åµæ¸¬åŸå§‹è³‡æ–™ç¼ºå¤± (Type A) ---
        min_time, max_time = df['datetime'].min(), df['datetime'].max()
        full_time_range = pd.date_range(start=min_time, end=max_time, freq='h')
        expected_len = len(full_time_range)

        # ä½¿ç”¨ä½è¨˜æ†¶é«” Loop
        for (site, item), group in df.groupby(['site', 'item']):
            if len(group) < expected_len:
                missing_report.append({
                    'file': file_name,
                    'site': site,
                    'item': item,
                    'missing_type': 'åŸå§‹è³‡æ–™ç¼ºå¤±',
                    'count': expected_len - len(group),
                    'note': 'è©²æ™‚æ®µç„¡æ•¸æ“š'
                })

        # --- 3. æº–å‚™è¨ˆç®— ---
        df['day_of_year'] = df['datetime'].dt.dayofyear.astype('int16')
        df['hour'] = df['datetime'].dt.hour.astype('int8')

        # --- 4. åµæ¸¬æ­·å²å¹³å‡ç¼ºå¤± (Type B) & è¨ˆç®— ---
        merged = pd.merge(
            df,
            global_avg_lookup,
            on=['item', 'site', 'day_of_year', 'hour'],
            how='left',
            indicator=True
        )

        # è¨˜éŒ„æ­·å²å¹³å‡ç¼ºå¤±
        missing_avg = merged[merged['_merge'] == 'left_only']
        if not missing_avg.empty:
            avg_summary = missing_avg.groupby(['site', 'item']).size().reset_index(name='count')
            for _, row in avg_summary.iterrows():
                missing_report.append({
                    'file': file_name,
                    'site': row['site'],
                    'item': row['item'],
                    'missing_type': 'æ­·å²å¹³å‡ç¼ºå¤±',
                    'count': row['count'],
                    'note': 'ç„¡æ­·å²å¹³å‡å€¼'
                })
        del missing_avg

        # --- 5. è¼¸å‡º Report (ç¨ç«‹æª”æ¡ˆ) ---
        if missing_report:
            df_rep = pd.DataFrame(missing_report)
            rep_path = os.path.join(report_dir, f"report_{file_name}.csv")
            df_rep.to_csv(rep_path, index=False, encoding='utf-8-sig')

        # --- 6. è¨ˆç®—è·å¹³ (åƒ…è¨ˆç®—è³‡æ–™å®Œæ•´çš„) ---
        final_data = merged[merged['_merge'] == 'both'].copy()
        del merged

        final_data['anomaly'] = final_data['value'] - final_data['avg_value']

        # dropna ç¢ºä¿æ²’æœ‰è¨ˆç®—å‡º NaN çš„çµæœ
        final_data = final_data.dropna(subset=['anomaly'])

        if final_data.empty: return None

        # è¼¸å‡º Raw
        df_out = final_data[['datetime', 'site', 'item', 'anomaly']].sort_values(['datetime', 'item', 'site'])
        raw_path = os.path.join(anomaly_dir, f"anomaly_{file_name}.csv")
        df_out.to_csv(raw_path, index=False, encoding='utf-8-sig')

        # è¼¸å‡º Region Avg
        final_data['region'] = final_data['site'].map(site_to_region)
        final_data = final_data.dropna(subset=['region'])

        if final_data.empty:
            del final_data, df_out
            return (raw_path, None)

        # å€åŸŸå¹³å‡è¨ˆç®— (SkipNA=True, éƒ¨åˆ†ç¼ºå¤±ç®—æœ‰çš„)
        region_avg = final_data.groupby(['datetime', 'item', 'region'])['anomaly'].mean().reset_index()
        region_avg['site'] = "AVG_" + region_avg['region']
        region_avg = region_avg.drop(columns=['region'])

        reg_path = os.path.join(anomaly_dir, f"region_avg_{file_name}.csv")
        region_avg.to_csv(reg_path, index=False, encoding='utf-8-sig')

        del final_data, region_avg, df_out
        gc.collect()

        return (raw_path, reg_path)

    except Exception as e:
        with open(os.path.join(report_dir, f"ERROR_{file_name}.txt"), "w") as f:
            f.write(str(e))
        return None

# ===============================================================
#   STEP 3: ç¹ªåœ–é‚è¼¯ (æ–·ç·šè™•ç†)
# ===============================================================

def plot_file_result(raw_csv, region_csv):
    if not region_csv or not os.path.exists(region_csv): return 0
    try:
        df_raw = pd.read_csv(raw_csv, usecols=['datetime', 'site', 'item', 'anomaly'])
        df_region = pd.read_csv(region_csv)

        df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])
        df_region['datetime'] = pd.to_datetime(df_region['datetime'])

        file_name = os.path.basename(raw_csv).replace("anomaly_", "").replace(".csv", "")
        save_dir = os.path.join(img_dir, file_name)
        os.makedirs(save_dir, exist_ok=True)

        # å–å¾—è©²æª”æ¡ˆçš„å…¨åŸŸæ™‚é–“ç¯„åœ (ç”¨æ–¼ Reindex æ–·ç·š)
        min_t = min(df_raw['datetime'].min(), df_region['datetime'].min())
        max_t = max(df_raw['datetime'].max(), df_region['datetime'].max())
        full_time_idx = pd.date_range(start=min_t, end=max_t, freq='h')

        plot_count = 0
        items = df_region['item'].unique()

        for item in items:
            item_info = items_info.get(item, {'name': item, 'unit': ''})

            raw_data = df_raw[df_raw['item'] == item]
            reg_data = df_region[df_region['item'] == item]

            if reg_data.empty: continue

            fig, ax = plt.subplots(figsize=(15, 8))

            # --- ç•«èƒŒæ™¯æ¸¬ç«™ (ç°ç·šï¼Œæ–·ç·šè™•ç†) ---
            for site, group in raw_data.groupby('site'):
                group_reindexed = group.set_index('datetime').reindex(full_time_idx)
                ax.plot(group_reindexed.index, group_reindexed['anomaly'],
                        color='gray', alpha=0.15, linewidth=1)

            # --- ç•«å€åŸŸå¹³å‡ (å½©ç·šï¼Œæ–·ç·šè™•ç†) ---
            region_colors = {'AVG_åŒ—': 'blue', 'AVG_ä¸­': 'green', 'AVG_å—': 'red', 'AVG_æ±': 'orange'}
            for site, group in reg_data.groupby('site'):
                color = region_colors.get(site, 'black')
                group_reindexed = group.set_index('datetime').reindex(full_time_idx)

                ax.plot(group_reindexed.index, group_reindexed['anomaly'],
                        color=color, linewidth=2.5,
                        label=site.replace('AVG_', '')+"éƒ¨")

            ax.axhline(0, color='black', linestyle='--', alpha=0.5)
            ax.set_title(f"{file_name} - {item_info['name']} ({item})", fontsize=16)
            ax.set_ylabel(f"è·å¹³å€¼ ({item_info['unit']})")
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d\n%H:%M'))
            ax.legend(loc='upper right')

            out_path = os.path.join(save_dir, f"ANOMALY_{item}.png")
            plt.savefig(out_path, dpi=100)
            plt.close(fig)
            plt.clf()
            plot_count += 1

        del df_raw, df_region
        gc.collect()
        return plot_count
    except Exception as e:
        print(f"Plot Error: {e}")
        return 0

# ===============================================================
#   ä¸»æµç¨‹
# ===============================================================

def main():
    print("ğŸš€ å•Ÿå‹•å·¥å…· (ç¨ç«‹å ±å‘Š & æ–·ç·šç¹ªåœ–ç‰ˆ)")

    df_avg_lookup = load_and_transform_averages()
    if df_avg_lookup is None: return

    files = sorted(glob.glob(os.path.join(base_dir, "hourly_*.csv")))
    processed_results = []

    # [å¾©åŸé€²åº¦æ¢é¢¨æ ¼] åŠ å…¥ desc å’Œ unit
    print("âš¡ è¨ˆç®—èˆ‡ç”Ÿæˆå ±å‘Šä¸­...")
    with ProcessPoolExecutor(max_workers=N_WORKERS, initializer=init_worker, initargs=(df_avg_lookup,)) as executor:
        future_to_file = {executor.submit(process_single_file, f): f for f in files}

        for future in tqdm(as_completed(future_to_file), total=len(files), desc="è¨ˆç®—èˆ‡ç”Ÿæˆå ±å‘Š", unit="file"):
            result = future.result()
            if result: processed_results.append(result)
            gc.collect()

    del df_avg_lookup
    gc.collect()

    print("\nğŸ¨ ç¹ªåœ–ä¸­ (è‡ªå‹•æ–·é–‹ç¼ºå¤±éƒ¨åˆ†)...")
    total_plots = 0
    with ProcessPoolExecutor(max_workers=N_WORKERS) as executor:
        future_to_plot = {executor.submit(plot_file_result, raw, region): raw for raw, region in processed_results}

        # [å¾©åŸé€²åº¦æ¢é¢¨æ ¼] åŠ å…¥ desc å’Œ unit
        for future in tqdm(as_completed(future_to_plot), total=len(processed_results), desc="ç¹ªè£½åœ–è¡¨", unit="file"):
            total_plots += future.result()
            gc.collect()

    print(f"\nâœ… å®Œæˆï¼å…±ç”¢å‡º {total_plots} å¼µåœ–è¡¨ã€‚")

if __name__ == '__main__':
    main()
